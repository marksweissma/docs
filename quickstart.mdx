---
title: Quick Start
description: Get up and running with Moxn in 5 minutes
---

This guide walks you through installing the SDK, fetching a prompt, and making your first LLM call with telemetry.

## Prerequisites

- Python 3.10+
- A Moxn account and API key (get one at [moxn.dev](https://moxn.dev))
- An LLM provider API key (Anthropic, OpenAI, or Google)

## Installation

Install the Moxn SDK using pip:

```bash
pip install moxn
```

Set your API key as an environment variable:

```bash
export MOXN_API_KEY="your-api-key"
```

## Your First Moxn Call

Here's a complete example that fetches a prompt, creates a session, sends it to Claude, and logs the interaction:

<CodeGroup>

```python Anthropic
import asyncio
from moxn import MoxnClient
from moxn_types.content import Provider
from anthropic import Anthropic

async def main():
    # Initialize the Moxn client
    async with MoxnClient() as client:
        # Fetch a prompt from your task
        # Use branch_name for development, commit_id for production
        prompt = await client.get_prompt(
            prompt_id="your-prompt-id",
            branch_name="main"
        )

        # Create a session with your input data
        session = await client.create_prompt_session(
            prompt_id="your-prompt-id",
            branch_name="main",
            session_data=YourPromptInput(
                ragItems=[
                    ragItem(
                        title="Best Document",
                        content="The answer to the user's query"
                    )
                ]
            )
        )

        # Convert to Anthropic format and send
        anthropic = Anthropic()
        response = anthropic.messages.create(
            **session.to_anthropic_invocation()
        )

        print(response.content[0].text)

        # Log telemetry
        async with client.span(session) as span:
            await client.log_telemetry_event_from_response(
                session, response, Provider.ANTHROPIC
            )

asyncio.run(main())
```

```python OpenAI
import asyncio
from moxn import MoxnClient
from moxn_types.content import Provider
from openai import OpenAI

async def main():
    # Initialize the Moxn client
    async with MoxnClient() as client:
        # Fetch a prompt from your task
        prompt = await client.get_prompt(
            prompt_id="your-prompt-id",
            branch_name="main"
        )

        # Create a session with your input data
        session = await client.create_prompt_session(
            prompt_id="your-prompt-id",
            branch_name="main",
            session_data=None  # Replace with your codegen'd model
        )

        # Convert to OpenAI format and send
        openai = OpenAI()
        response = openai.chat.completions.create(
            **session.to_openai_chat_invocation()
        )

        print(response.choices[0].message.content)

        # Log telemetry
        async with client.span(session) as span:
            await client.log_telemetry_event_from_response(
                session, response, Provider.OPENAI_CHAT
            )

asyncio.run(main())
```

```python Google
import asyncio
from moxn import MoxnClient
from moxn_types.content import Provider
from google import genai

async def main():
    # Initialize the Moxn client
    async with MoxnClient() as client:
        # Create a session with your input data
        session = await client.create_prompt_session(
            prompt_id="your-prompt-id",
            branch_name="main",
            session_data=None  # Replace with your codegen'd model
        )

        # Convert to Google format and send
        google_client = genai.Client()
        response = google_client.models.generate_content(
            **session.to_google_gemini_invocation()
        )

        print(response.text)

        # Log telemetry
        async with client.span(session) as span:
            await client.log_telemetry_event_from_response(
                session, response, Provider.GOOGLE_GEMINI
            )

asyncio.run(main())
```

</CodeGroup>

## From Template to Provider Call

The code above follows the core Moxn pattern (see [Core Concepts](/index#core-concepts)):

| Step | What Happens |
|------|--------------|
| **1. Fetch template** | Your prompt (messages, variables, config) is retrieved from Moxn |
| **2. Create session** | Template + your `session_data` = a session ready to render |
| **3. Build invocation** | `to_anthropic_invocation()` returns a plain Python dict |
| **4. Call provider** | You pass the dict to the provider SDK with `**invocation` |

### You Control the Payload

The invocation is just a dictionary—you can inspect, modify, or extend it:

```python
invocation = session.to_anthropic_invocation()

# Add streaming
invocation["stream"] = True

# Use a new provider feature
invocation["extra_headers"] = {"anthropic-beta": "citations-2025-01-01"}

# Override settings for A/B testing
invocation["model"] = "claude-sonnet-4-20250514"

response = anthropic.messages.create(**invocation)
```

This design is intentional: Moxn helps you **build payloads**, but never **owns the integration**. You always call the provider SDK directly—no wrapper, no middleware, no magic. If a provider releases a new feature tomorrow, you can use it immediately by adding parameters to the dict or to the provider directly.

## Understanding the Code

Let's break down what's happening:

### 1. MoxnClient as Context Manager

```python
async with MoxnClient() as client:
```

The client manages connections and telemetry batching. Always use it as an async context manager to ensure proper cleanup.

### 2. Fetching Prompts

```python
prompt = await client.get_prompt(
    prompt_id="your-prompt-id",
    branch_name="main"  # or commit_id="abc123" for production
)
```

- **Branch access** (`branch_name`): Gets the latest version, including uncommitted changes. Use for development.
- **Commit access** (`commit_id`): Gets an immutable snapshot. Use for production.

### 3. Creating Sessions

```python
session = await client.create_prompt_session(
    prompt_id="your-prompt-id",
    branch_name="main",
    session_data=YourInputModel(...)  # Pydantic model from codegen
)
```

A session combines your prompt template with runtime data. The `session_data` is typically a Pydantic model generated by codegen.

### 4. Converting to Provider Format

```python
response = anthropic.messages.create(
    **session.to_anthropic_invocation()
)
```

The `to_*_invocation()` methods return complete payloads you can unpack directly into provider SDKs. They include:

- Messages formatted for the provider
- Model configuration from your prompt
- Tools and structured output schemas (if configured)

### 5. Logging Telemetry

```python
async with client.span(session) as span:
    await client.log_telemetry_event_from_response(
        session, response, Provider.ANTHROPIC
    )
```

Spans create observable traces of your LLM calls. Every call within a span is linked for debugging and analysis.

## Using Code Generation

For type-safe session data, generate Pydantic models from your prompts:

```python
# Generate models for all prompts in a task
async with MoxnClient() as client:
    await client.generate_task_models(
        task_id="your-task-id",
        branch_name="main",
        output_dir="./models"
    )
```

This creates a Python file with Pydantic models matching your prompt's input schema:

```python
# models/your_task_models.py (generated)
from datetime import datetime
from pydantic import BaseModel
from moxn_types.base import RenderableModel

class Document(BaseModel):
    title: str
    created_at: datetime
    last_modified: datetime | None = None
    author: str
    content: str

    def to_xml(self) -> str:
        """Render document as XML with metadata as attributes."""
        attrs = f'title="{self.title}" author="{self.author}" created_at="{self.created_at.isoformat()}"'
        if self.last_modified:
            attrs += f' last_modified="{self.last_modified.isoformat()}"'
        return f"<document {attrs}>\n{self.content}\n</document>"


class YourPromptInput(RenderableModel):
    query: str
    user_id: str
    context: list[Document] | None = None

    def render(self, **kwargs) -> dict[str, str]:
        # Render context as XML document collection
        if self.context:
            docs_xml = "\n".join(doc.to_xml() for doc in self.context)
            context_str = f"<context>\n{docs_xml}\n</context>"
        else:
            context_str = ""

        return {
            "query": self.query,
            "user_id": self.user_id,
            "context": context_str,
        }
```

The `render()` method transforms your typed data into strings for prompt injection. This example renders documents as XML—a format that works well for providing structured context to LLMs.

Then use it in your code:

```python
from models.your_task_models import YourPromptInput, Document
from datetime import datetime

session = await client.create_prompt_session(
    prompt_id="your-prompt-id",
    session_data=YourPromptInput(
        query="How do I reset my password?",
        user_id="user_123",
        context=[
            Document(
                title="Password Reset Guide",
                created_at=datetime(2024, 1, 15),
                author="Support Team",
                content="To reset your password, click 'Forgot Password' on the login page..."
            ),
            Document(
                title="Account Security FAQ",
                created_at=datetime(2024, 2, 1),
                last_modified=datetime(2024, 3, 10),
                author="Security Team",
                content="We recommend using a password manager and enabling 2FA..."
            )
        ]
    )
)
```

This renders into the prompt as:

```xml
<context>
<document title="Password Reset Guide" author="Support Team" created_at="2024-01-15T00:00:00">
To reset your password, click 'Forgot Password' on the login page...
</document>
<document title="Account Security FAQ" author="Security Team" created_at="2024-02-01T00:00:00" last_modified="2024-03-10T00:00:00">
We recommend using a password manager and enabling 2FA...
</document>
</context>
```

## Next Steps

<CardGroup cols={2}>
  <Card title="The Moxn Workflow" icon="arrows-rotate" href="/guides/workflow">
    Understand the full development workflow
  </Card>
  <Card title="Prompt Sessions" icon="layer-group" href="/guides/sessions">
    Learn more about sessions and rendering
  </Card>
  <Card
    title="Code Generation"
    icon="wand-magic-sparkles"
    href="/guides/codegen"
  >
    Deep dive into type-safe model generation
  </Card>
  <Card title="Telemetry" icon="chart-line" href="/telemetry/spans">
    Set up comprehensive observability
  </Card>
</CardGroup>
