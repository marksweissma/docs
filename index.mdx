---
title: Introduction
description: Versioned prompt management for production AI systems
---

# Moxn SDK

Moxn is a Git-inspired content management system for AI engineering teams. It brings version control semantics to prompt management, enabling teams to collaborate on prompts with branching, merging, and rollback capabilities—just like code.

## The Problem

Prompts are code, but they don't have the tooling.

When building AI applications, you face a familiar set of challenges:

- **No version control**: Prompts live in code comments, config files, or databases with no history
- **No type safety**: Input variables are stringly-typed, leading to runtime errors
- **No observability**: You can't see what prompts actually ran in production
- **No collaboration**: No shared workflows for reviewing prompts or debugging production traces

Moxn solves these problems by treating prompts as first-class versioned entities with type-safe interfaces and full observability.

## How Moxn Works

<Steps>
  <Step title="Create prompts in the web app">
    Design prompts with images, PDFs, and variables. Define variables using your backend types—Moxn provides render hooks to transform rich data into LLM-optimized formats.
  </Step>
  <Step title="Generate type-safe models">
    Run codegen to create Pydantic models for your prompts. Get autocomplete and validation.
  </Step>
  <Step title="Use in your code">
    Fetch prompts, create sessions with your data, and send to any LLM provider. Or push existing prompts from your codebase to Moxn.
  </Step>
  <Step title="Log telemetry">
    Log LLM calls with full context—input, output, and trace hierarchy. You control what gets logged.
  </Step>
</Steps>

## Key Features

<CardGroup cols={2}>
  <Card title="Git-Like Versioning" icon="code-branch">
    Branch, commit, and rollback prompts. Pin production to specific commits.
  </Card>
  <Card title="Type-Safe Interfaces" icon="shield-check">
    Auto-generated Pydantic models ensure type safety from editor to runtime.
  </Card>
  <Card title="Multi-Provider Support" icon="server">
    Convert prompts to Anthropic, OpenAI, Google, or any provider format.
  </Card>
  <Card title="Full Observability" icon="chart-line">
    W3C Trace Context compatible spans with complete LLM event logging.
  </Card>
</CardGroup>

## Quick Example

```python
from moxn import MoxnClient
from moxn_types.content import Provider
from anthropic import Anthropic

# Define your session data (generated via codegen)
session_data = YourInputModel(
    query="How do I reset my password?",
    user_id="user_123"
)

# Fetch a prompt and create a session
async with MoxnClient() as client:
    session = await client.create_prompt_session(
        prompt_id="your-prompt-id",
        session_data=session_data
    )

    # Convert to Anthropic format and send
    anthropic = Anthropic()
    response = anthropic.messages.create(
        **session.to_anthropic_invocation()
    )

    # Log the interaction
    async with client.span(session) as span:
        await client.log_telemetry_event_from_response(
            session, response, Provider.ANTHROPIC
        )
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Quick Start" icon="rocket" href="/quickstart">
    Get up and running in 5 minutes
  </Card>
  <Card title="The Moxn Workflow" icon="arrows-rotate" href="/guides/workflow">
    Understand the end-to-end workflow
  </Card>
  <Card title="Core Concepts" icon="book" href="/concepts/entities">
    Learn about Tasks, Prompts, and Messages
  </Card>
  <Card title="Examples" icon="code" href="/examples/rag">
    See complete working examples
  </Card>
</CardGroup>
