---
title: Quick Start
description: Get up and running with Moxn in 5 minutes
---

This guide walks you through installing the SDK, fetching a prompt, and making your first LLM call with telemetry.

## Prerequisites

- Python 3.10+
- A Moxn account and API key (get one at [moxn.dev](https://moxn.dev))
- An LLM provider API key (Anthropic, OpenAI, or Google)

## Installation

Install the Moxn SDK using pip:

```bash
pip install moxn
```

Set your API key as an environment variable:

```bash
export MOXN_API_KEY="your-api-key"
```

## Your First Moxn Call

Here's a complete example that fetches a prompt, creates a session, sends it to Claude, and logs the interaction:

<CodeGroup>

```python Anthropic
import asyncio
from moxn import MoxnClient
from moxn_types.content import Provider
from anthropic import Anthropic

async def main():
    # Initialize the Moxn client
    async with MoxnClient() as client:
        # Fetch a prompt from your task
        # Use branch_name for development, commit_id for production
        prompt = await client.get_prompt(
            prompt_id="your-prompt-id",
            branch_name="main"
        )

        # Create a session with your input data
        session = await client.create_prompt_session(
            prompt_id="your-prompt-id",
            branch_name="main",
            session_data=YourPromptInput(
                ragItems=[
                    ragItem(
                        title="Best Document",
                         content="The answer to the user's query"
                    )
                ]
            )
        )

        # Convert to Anthropic format and send
        anthropic = Anthropic()
        response = anthropic.messages.create(
            **session.to_anthropic_invocation()
        )

        print(response.content[0].text)

        # Log telemetry
        async with client.span(session) as span:
            await client.log_telemetry_event_from_response(
                session, response, Provider.ANTHROPIC
            )

asyncio.run(main())
```

```python OpenAI
import asyncio
from moxn import MoxnClient
from moxn_types.content import Provider
from openai import OpenAI

async def main():
    # Initialize the Moxn client
    async with MoxnClient() as client:
        # Fetch a prompt from your task
        prompt = await client.get_prompt(
            prompt_id="your-prompt-id",
            branch_name="main"
        )

        # Create a session with your input data
        session = await client.create_prompt_session(
            prompt_id="your-prompt-id",
            branch_name="main",
            session_data=None  # Replace with your codegen'd model
        )

        # Convert to OpenAI format and send
        openai = OpenAI()
        response = openai.chat.completions.create(
            **session.to_openai_chat_invocation()
        )

        print(response.choices[0].message.content)

        # Log telemetry
        async with client.span(session) as span:
            await client.log_telemetry_event_from_response(
                session, response, Provider.OPENAI_CHAT
            )

asyncio.run(main())
```

```python Google
import asyncio
from moxn import MoxnClient
from moxn_types.content import Provider
from google import genai

async def main():
    # Initialize the Moxn client
    async with MoxnClient() as client:
        # Create a session with your input data
        session = await client.create_prompt_session(
            prompt_id="your-prompt-id",
            branch_name="main",
            session_data=None  # Replace with your codegen'd model
        )

        # Convert to Google format and send
        google_client = genai.Client()
        response = google_client.models.generate_content(
            **session.to_google_gemini_invocation()
        )

        print(response.text)

        # Log telemetry
        async with client.span(session) as span:
            await client.log_telemetry_event_from_response(
                session, response, Provider.GOOGLE_GEMINI
            )

asyncio.run(main())
```

</CodeGroup>

## From Template to Provider Call

The code above follows the core Moxn pattern (see [Core Concepts](/index#core-concepts)):

| Step | What Happens |
|------|--------------|
| **1. Fetch template** | Your prompt (messages, variables, config) is retrieved from Moxn |
| **2. Create session** | Template + your `session_data` = a session ready to render |
| **3. Build invocation** | `to_anthropic_invocation()` returns a plain Python dict |
| **4. Call provider** | You pass the dict to the provider SDK with `**invocation` |

### You Control the Payload

The invocation is just a dictionary—you can inspect, modify, or extend it:

```python
invocation = session.to_anthropic_invocation()

# Add streaming
invocation["stream"] = True

# Use a new provider feature
invocation["extra_headers"] = {"anthropic-beta": "citations-2025-01-01"}

# Override settings for A/B testing
invocation["model"] = "claude-sonnet-4-20250514"

response = anthropic.messages.create(**invocation)
```

This design is intentional: Moxn helps you **build payloads**, but never **owns the integration**. You always call the provider SDK directly—no wrapper, no middleware, no magic. If a provider releases a new feature tomorrow, you can use it immediately by adding parameters to the dict.

## Understanding the Code

Let's break down what's happening:

### 1. MoxnClient as Context Manager

```python
async with MoxnClient() as client:
```

The client manages connections and telemetry batching. Always use it as an async context manager to ensure proper cleanup.

### 2. Fetching Prompts

```python
prompt = await client.get_prompt(
    prompt_id="your-prompt-id",
    branch_name="main"  # or commit_id="abc123" for production
)
```

- **Branch access** (`branch_name`): Gets the latest version, including uncommitted changes. Use for development.
- **Commit access** (`commit_id`): Gets an immutable snapshot. Use for production.

### 3. Creating Sessions

```python
session = await client.create_prompt_session(
    prompt_id="your-prompt-id",
    branch_name="main",
    session_data=YourInputModel(...)  # Pydantic model from codegen
)
```

A session combines your prompt template with runtime data. The `session_data` is typically a Pydantic model generated by codegen.

### 4. Converting to Provider Format

```python
response = anthropic.messages.create(
    **session.to_anthropic_invocation()
)
```

The `to_*_invocation()` methods return complete payloads you can unpack directly into provider SDKs. They include:

- Messages formatted for the provider
- Model configuration from your prompt
- Tools and structured output schemas (if configured)

### 5. Logging Telemetry

```python
async with client.span(session) as span:
    await client.log_telemetry_event_from_response(
        session, response, Provider.ANTHROPIC
    )
```

Spans create observable traces of your LLM calls. Every call within a span is linked for debugging and analysis.

## Using Code Generation

For type-safe session data, generate Pydantic models from your prompts:

```python
# Generate models for all prompts in a task
async with MoxnClient() as client:
    await client.generate_task_models(
        task_id="your-task-id",
        branch_name="main",
        output_dir="./models"
    )
```

This creates a Python file with Pydantic models matching your prompt's input schema:

```python
# models/your_task_models.py (generated)
from moxn_types.base import RenderableModel

class YourPromptInput(RenderableModel):
    query: str
    user_id: str
    context: list[str] | None = None

    def render(self, **kwargs) -> dict[str, str]:
        # Transforms your data for prompt injection
        return {
            "query": self.query,
            "user_id": self.user_id,
            "context": json.dumps(self.context) if self.context else "",
        }
```

Then use it in your code:

```python
from models.your_task_models import YourPromptInput

session = await client.create_prompt_session(
    prompt_id="your-prompt-id",
    session_data=YourPromptInput(
        query="How do I reset my password?",
        user_id="user_123",
        context=["Previous support tickets..."]
    )
)
```

## Next Steps

<CardGroup cols={2}>
  <Card title="The Moxn Workflow" icon="arrows-rotate" href="/guides/workflow">
    Understand the full development workflow
  </Card>
  <Card title="Prompt Sessions" icon="layer-group" href="/guides/sessions">
    Learn more about sessions and rendering
  </Card>
  <Card
    title="Code Generation"
    icon="wand-magic-sparkles"
    href="/guides/codegen"
  >
    Deep dive into type-safe model generation
  </Card>
  <Card title="Telemetry" icon="chart-line" href="/telemetry/spans">
    Set up comprehensive observability
  </Card>
</CardGroup>
